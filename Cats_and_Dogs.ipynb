{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan-S-S/Oficina-de-CNN/blob/main/Cats_and_Dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RB6KNJ8bDwl1"
      },
      "outputs": [],
      "source": [
        "# Instalar bibliotecas necessárias (se não tiver)\n",
        "# !pip install pandas torch torchvision matplotlib scikit-learn opencv-python\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAarQRPqrK3S"
      },
      "source": [
        "#**Dataset**: (definição, organização e pré processamento)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baixa o dataset compactado e o extrai."
      ],
      "metadata": {
        "id": "ItN5O7daEX40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmcCsSohD2Cc",
        "outputId": "540f9a01-49fb-4632-ebbe-f6c2b462c438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando o dataset...\n",
            "Arquivo zip já existe.\n",
            "Pasta PetImages já existe.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Baixar e Extrair o Dataset ---\n",
        "# Se você estiver rodando em Google Colab ou ambiente similar, isso funciona.\n",
        "# Se estiver em um script local, pode precisar adaptar.\n",
        "print(\"Baixando o dataset...\")\n",
        "# Verifica se o arquivo zip já existe para evitar download duplicado\n",
        "if not os.path.exists('kagglecatsanddogs_5340.zip'):\n",
        "    !wget https://download.microsoft.com/download/3/e/1/3e1c3f21-ecdb-4869-8368-6deba77b919f/kagglecatsanddogs_5340.zip\n",
        "    print(\"Download completo.\")\n",
        "else:\n",
        "    print(\"Arquivo zip já existe.\")\n",
        "\n",
        "# Verifica se a pasta PetImages já existe para evitar extração duplicada\n",
        "if not os.path.exists('PetImages'):\n",
        "    print(\"Extraindo o dataset...\")\n",
        "    with zipfile.ZipFile('kagglecatsanddogs_5340.zip', 'r') as zip_ref:\n",
        "        # Extrair apenas as pastas Cat e Dog, pois __MACOSX pode causar problemas\n",
        "        for member in zip_ref.namelist():\n",
        "            if \"PetImages/Cat/\" in member or \"PetImages/Dog/\" in member:\n",
        "                 zip_ref.extract(member, '.')\n",
        "    print(\"Extração completa.\")\n",
        "else:\n",
        "    print(\"Pasta PetImages já existe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Percorre as pastas e organiza as lista de imagem e label."
      ],
      "metadata": {
        "id": "HhoePKM-FBIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp0EBaNQF2Rv",
        "outputId": "3a1d34a5-20ac-427c-f888-dcfdd0c7220f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organizando os dados...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verificando Cats: 100%|██████████| 12501/12501 [00:01<00:00, 8585.66it/s]\n",
            "Verificando Dogs:  96%|█████████▌| 11970/12501 [00:01<00:00, 9053.91it/s]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Verificando Dogs: 100%|██████████| 12501/12501 [00:01<00:00, 8889.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total de imagens encontradas: 24998\n",
            "Total de imagens corrompidas ignoradas: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Organizar os Dados ---\n",
        "print(\"Organizando os dados...\")\n",
        "base_dir = 'PetImages'\n",
        "categories = ['Cat', 'Dog']\n",
        "image_paths = []\n",
        "labels = []\n",
        "corrupted_images = [] # Lista para armazenar caminhos de imagens corrompidas\n",
        "\n",
        "for category in categories:\n",
        "    path = os.path.join(base_dir, category)\n",
        "    # Usamos tqdm para ver o progresso ao listar e verificar imagens\n",
        "    for img_name in tqdm(os.listdir(path), desc=f\"Verificando {category}s\"):\n",
        "        img_path = os.path.join(path, img_name)\n",
        "        # --- 4. Tratar Imagens Corrompidas ---\n",
        "        try:\n",
        "            # Tenta abrir a imagem. Se der erro, é provável que esteja corrompida.\n",
        "            # O .verify() checa a integridade, mas pode não pegar todos os casos.\n",
        "            # Abrir e fechar é mais robusto.\n",
        "            img = Image.open(img_path)\n",
        "            img.verify() # Verifica a integridade do arquivo\n",
        "            image_paths.append(img_path)\n",
        "            labels.append(0 if category == 'Cat' else 1) # 0 para Gato, 1 para Cachorro\n",
        "        except (IOError, SyntaxError) as e:\n",
        "            #print(f\"Imagem corrompida ou ilegível: {img_path}. Erro: {e}\")\n",
        "            corrupted_images.append(img_path)\n",
        "            # Pass (ignora a imagem corrompida)\n",
        "\n",
        "print();\n",
        "print(f\"Total de imagens encontradas: {len(image_paths)}\")\n",
        "print(f\"Total de imagens corrompidas ignoradas: {len(corrupted_images)}\")\n",
        "\n",
        "# Criar um DataFrame pandas\n",
        "df = pd.DataFrame({'path': image_paths, 'label': labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separa oque vai ser Treino e Teste."
      ],
      "metadata": {
        "id": "em0CGYk-hz8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KljsEsiF56v",
        "outputId": "431d7af9-fa4e-47cd-ec65-e170df1676ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número de imagens para treino: 15998\n",
            "Número de imagens para validação: 5000\n",
            "\n",
            "Contagem de Gatos e Cachorros no conjunto de Treino:\n",
            "label\n",
            "Dog    7999\n",
            "Cat    7999\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Contagem de Gatos e Cachorros no conjunto de Validação:\n",
            "label\n",
            "Dog    2000\n",
            "Cat    2000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Contagem de Gatos e Cachorros no conjunto de Teste:\n",
            "label\n",
            "Dog    2500\n",
            "Cat    2500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Dividir os Dados ---\n",
        "# Usar train_test_split para dividir em treino e validação (ex: 80% treino, 20% validação)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
        "\n",
        "print(f\"\\nNúmero de imagens para treino: {len(train_df)}\")\n",
        "print(f\"Número de imagens para validação: {len(val_df)}\")\n",
        "print(f\"Número de imagens para teste: {len(test_df)}\")\n",
        "\n",
        "# Contar o número de gatos e cachorros em cada dataframe\n",
        "print(\"\\nContagem de Gatos e Cachorros no conjunto de Treino:\")\n",
        "print(train_df['label'].value_counts().rename(index={0: 'Cat', 1: 'Dog'}))\n",
        "\n",
        "print(\"\\nContagem de Gatos e Cachorros no conjunto de Validação:\")\n",
        "print(val_df['label'].value_counts().rename(index={0: 'Cat', 1: 'Dog'}))\n",
        "\n",
        "print(\"\\nContagem de Gatos e Cachorros no conjunto de Teste:\")\n",
        "print(test_df['label'].value_counts().rename(index={0: 'Cat', 1: 'Dog'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Classe **PetImagesDataset** para facilitar o treinamento."
      ],
      "metadata": {
        "id": "IkaE4d0Zh0pr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EWEUUrPnF8r_"
      },
      "outputs": [],
      "source": [
        "# --- 5. Criar um Dataset Customizado ---\n",
        "class PetImagesDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx, 0]\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "\n",
        "        try:\n",
        "            # Abrir a imagem\n",
        "            image = Image.open(img_path).convert('RGB') # Garantir que é RGB (3 canais)\n",
        "            # Aplicar transformações\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "             #print(f\"Erro ao carregar/transformar imagem {img_path}: {e}\")\n",
        "             return None, None # Retorna None para sinalizar problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define e aplica as transformações de Pré processamento nas imagens."
      ],
      "metadata": {
        "id": "0NjJPssgh1Zc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "U6sbFdpiGBER"
      },
      "outputs": [],
      "source": [
        "# --- 6. Definir Transformações ---\n",
        "# Transformações para treino (inclui aumento de dados)\n",
        "# Resize para um tamanho um pouco maior, depois RandomCrop para o tamanho final\n",
        "# ToTensor converte a imagem PIL para Tensor e escala os pixels para [0, 1]\n",
        "# Normalize usa médias e desvios padrões comuns (ex: ImageNet)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((70, 70)),\n",
        "    transforms.RandomCrop((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(), # Aumento de dados: vira horizontalmente aleatoriamente\n",
        "    transforms.ToTensor(),\n",
        "    # Valores de normalização comuns (médias e desvios padrões por canal RGB)\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Transformações para teste (apenas redimensionar e normalizar)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)), # Redimensiona diretamente para o tamanho final\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Transformações para validação (apenas redimensionar e normalizar)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)), # Redimensiona diretamente para o tamanho final\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "# Criar instâncias dos Datasets\n",
        "train_dataset = PetImagesDataset(train_df, transform=train_transform)\n",
        "val_dataset = PetImagesDataset(val_df, transform=val_transform)\n",
        "test_dataset = PetImagesDataset(test_df, transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define os DataLoaders"
      ],
      "metadata": {
        "id": "Nz1To4crA3yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A5_hseqfGD7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197b4e56-b14d-45a1-a6d6-31668f4886ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Criar DataLoaders ---\n",
        "# DataLoaders ajudam a carregar os dados em batches e embaralhar (no treino)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Função para lidar com amostras None no DataLoader (devido a imagens corrompidas tratadas)\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch)) # Remove None tuples\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H0nBi4qrK3b"
      },
      "source": [
        "# **Construir o Modelo CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "59J_ksO6GLCx"
      },
      "outputs": [],
      "source": [
        "# --- 8. Definir o Modelo (CNN Simples) ---\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=1): # 1 classe de saída para classificação binária com BCEWithLogitsLoss\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Camada Convolucional 1\n",
        "        # Entrada: 3 canais (RGB), Saída: 32 canais, Kernel: 3x3\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Diminui a dimensão espacial pela metade\n",
        "\n",
        "        # Camada Convolucional 2\n",
        "        # Entrada: 32 canais, Saída: 64 canais, Kernel: 3x3\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Diminui a dimensão espacial pela metade\n",
        "\n",
        "        # Camada Convolucional 3\n",
        "        # Entrada: 64 canais, Saída: 128 canais, Kernel: 3x3\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # Diminui a dimensão espacial pela metade\n",
        "\n",
        "        # Camadas Totalmente Conectadas\n",
        "        # Após as camadas de pooling, a dimensão da imagem 64x64 -> 32x32 -> 16x16 -> 8x8\n",
        "        # Temos 128 canais, então 128 * 8 * 8 = 8192 features antes da primeira camada FC\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5) # Dropout para regularização\n",
        "        self.fc2 = nn.Linear(512, num_classes) # Saída: 1 neurônio para classificação binária\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.pool3(self.relu3(self.conv3(x)))\n",
        "\n",
        "        # Achatar (flatten) os dados antes das camadas totalmente conectadas\n",
        "        x = x.view(-1, 128 * 8 * 8) # -1 infere o tamanho do batch size\n",
        "\n",
        "        x = self.dropout(self.relu4(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instanciar o modelo\n",
        "model = SimpleCNN(num_classes=1)\n",
        "\n",
        "# --- Configurar Dispositivo (CPU ou GPU) ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# --- 8. Configurar Função de Perda e Otimizador ---\n",
        "# BCEWithLogitsLoss é boa para classificação binária, combina Sigmoid + Binary Cross Entropy\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Otimizador Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfHjhqZprK3d"
      },
      "source": [
        "# **Treinamento**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenta carregar o modelo já treinado."
      ],
      "metadata": {
        "id": "m8FtCI9MqCF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykB1ir-dGO9G",
        "outputId": "1d9e37e9-9ab9-4a4e-b305-987d2464c583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo de modelo encontrado em best_cats_vs_dogs_model.pth. Carregando...\n",
            "Modelo e otimizador carregados com sucesso. Retomando o treinamento a partir da Época 10.\n",
            "Melhor acurácia de validação prévia: 0.8767\n"
          ]
        }
      ],
      "source": [
        "# --- 9. Treinar o Modelo ---\n",
        "# Se você carregar um modelo salvo na época 5 e NUM_EPOCHS for 10, ele rodará até a época 5 (totalizando 10).\n",
        "TOTAL_NUM_EPOCHS = 10\n",
        "NUM_EPOCHS = TOTAL_NUM_EPOCHS\n",
        "\n",
        "# --- Configurar Salvamento do Melhor Modelo ---\n",
        "best_model_path = 'best_cats_vs_dogs_model.pth' # Nome do arquivo onde o modelo será salvo/carregado\n",
        "\n",
        "# Inicializar a época de início do treinamento\n",
        "start_epoch = 0\n",
        "# Inicializar a melhor acurácia (será substituída se um modelo for carregado)\n",
        "best_accuracy = -1.0 # Inicializa aqui, antes da tentativa de carregar\n",
        "\n",
        "# --- Lógica para Carregar o Modelo Salvo (se existir) ---\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Arquivo de modelo encontrado em {best_model_path}. Carregando...\")\n",
        "    try:\n",
        "        # Carregar o checkpoint (inclui estado do modelo, otimizador, época e melhor acurácia)\n",
        "        checkpoint = torch.load(best_model_path, map_location=device)\n",
        "\n",
        "        # Carregar o estado do modelo\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Carregar o estado do otimizador (importante para continuar o treinamento)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Carregar a melhor acurácia salva\n",
        "        best_accuracy = checkpoint['best_accuracy']\n",
        "\n",
        "        # Definir a época de início para continuar de onde parou\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "        print(f\"Modelo e otimizador carregados com sucesso. Retomando o treinamento a partir da Época {start_epoch}.\")\n",
        "        print(f\"Melhor acurácia de validação prévia: {best_accuracy:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo de {best_model_path}. Começando do zero. Erro: {e}\")\n",
        "        # Se houver erro ao carregar, start_epoch e best_accuracy permanecem nos valores iniciais (0 e -1.0)\n",
        "else:\n",
        "    print(f\"Nenhum arquivo de modelo encontrado em {best_model_path}. Iniciando treinamento do zero.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O fluxo do treinamento."
      ],
      "metadata": {
        "id": "CyZqVggsqIkZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vGnQil7crK3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b450f86-99ab-405b-b0d0-f1dd8010afc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento...\n",
            "\n",
            "Treinamento finalizado!\n"
          ]
        }
      ],
      "source": [
        "# --- Treinamento---\n",
        "print(\"\\nIniciando o treinamento...\")\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    model.train() # Coloca o modelo em modo de treino (habilita dropout, etc.)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Loop sobre os batches do treino\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Época {epoch+1}/{NUM_EPOCHS} [Treino]\"): # Ajusta o desc para mostrar o número correto da época\n",
        "        # Mover dados para o dispositivo correto (CPU/GPU)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1) # labels precisam ser float e ter a mesma dimensão da saída do modelo\n",
        "\n",
        "        # Zerar os gradientes do otimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calcular a perda\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass e otimização\n",
        "        loss.backward() # Calcula os gradientes\n",
        "        optimizer.step() # Atualiza os pesos\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # --- Avaliação no conjunto de validação após cada época ---\n",
        "    model.eval() # Coloca o modelo em modo de avaliação (desabilita dropout, etc.)\n",
        "    val_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(): # Desabilita o cálculo de gradientes na avaliação\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Época {epoch+1}/{NUM_EPOCHS} [Validação]\"): # ADIÇÃO: Ajusta o desc para mostrar o número correto da época\n",
        "            # Mover dados para o dispositivo correto\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calcular a perda de validação\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calcular acurácia\n",
        "            predicted = torch.round(torch.sigmoid(outputs))\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "\n",
        "    # --- Lógica para Salvar o Melhor Modelo (Checkpoint Completo) ---\n",
        "    # Se a acurácia nesta época for maior que a melhor acurácia vista até agora\n",
        "    if epoch_accuracy > best_accuracy:\n",
        "        best_accuracy = epoch_accuracy # Atualiza a melhor acurácia\n",
        "        # Salva um dicionário (checkpoint) com mais informações\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_accuracy': best_accuracy\n",
        "        }\n",
        "        torch.save(checkpoint, best_model_path)\n",
        "        print(f\"-> Salvo novo melhor modelo (checkpoint) em {best_model_path} com Acurácia de Validação: {best_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    print(f\"Época [{epoch+1}/{NUM_EPOCHS}], Perda de Treino: {epoch_loss:.4f}, Perda de Validação: {epoch_val_loss:.4f}, Acurácia de Validação: {epoch_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTreinamento finalizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW7_pwcerK3f"
      },
      "source": [
        "# **Avaliação e Métricas**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6xC4X6mpzxJ",
        "outputId": "84088534-b69b-43fc-c5a6-9d18f2bcef14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPI6iGXkDdtf",
        "outputId": "e6c06d65-82ce-4aed-8a1d-c50171317d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliação final no conjunto de validação...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avaliando:  73%|███████▎  | 58/79 [00:26<00:07,  2.83it/s]/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Avaliando: 100%|██████████| 79/79 [00:33<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perda Final de Validação: 0.2832\n",
            "Acurácia Final de Validação: 0.8772\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Cat       0.86      0.90      0.88      2500\n",
            "         Dog       0.90      0.85      0.87      2500\n",
            "\n",
            "    accuracy                           0.88      5000\n",
            "   macro avg       0.88      0.88      0.88      5000\n",
            "weighted avg       0.88      0.88      0.88      5000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 10. Avaliação Final ---\n",
        "print(\"\\nAvaliação final no conjunto de validação...\")\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Avaliando\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "final_val_loss = val_loss / len(test_loader.dataset)\n",
        "final_accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f\"Perda Final de Validação: {final_val_loss:.4f}\")\n",
        "print(f\"Acurácia Final de Validação: {final_accuracy:.4f}\")\n",
        "\n",
        "# Você pode gerar um relatório de classificação mais detalhado se quiser\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=['Cat', 'Dog']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwzMiXHprK3g"
      },
      "source": [
        "# **Uso de Imagem própria**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLP7SLrQZLPD",
        "outputId": "3bfd498a-4238-407d-f909-9ce2c1d65509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erro: Arquivo não encontrado em /content/cachorro.jpg\n"
          ]
        }
      ],
      "source": [
        "# --- Bloco para Testar com uma Foto Única ---\n",
        "\n",
        "# 1. Defina o caminho para a sua foto\n",
        "caminho_da_sua_foto = '/content/cachorro.jpg'\n",
        "\n",
        "# Verifique se o arquivo existe\n",
        "if not os.path.exists(caminho_da_sua_foto):\n",
        "    print(f\"Erro: Arquivo não encontrado em {caminho_da_sua_foto}\")\n",
        "else:\n",
        "    try:\n",
        "        # 2. Carregar a imagem\n",
        "        imagem = Image.open(caminho_da_sua_foto).convert('RGB') # Garante 3 canais\n",
        "\n",
        "        # Exibir a imagem original\n",
        "        print(\"Imagem Original:\")\n",
        "        display(imagem)\n",
        "\n",
        "\n",
        "        # 3. Aplicar as transformações de validação\n",
        "        imagem_tensor = val_transform(imagem)\n",
        "\n",
        "        # ADIÇÃO: Exibir a imagem após as transformações (como tensor)\n",
        "        # Para exibir, precisamos converter de volta para PIL Image.\n",
        "        # Primeiro, desnormalizamos.\n",
        "        # Os valores de normalização foram (0.485, 0.456, 0.406) e (0.229, 0.224, 0.225)\n",
        "        # Imagem_normalizada = (Imagem_original - Média) / StdDev\n",
        "        # Imagem_original = Imagem_normalizada * StdDev + Média\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        imagem_denormalizada = imagem_tensor * std + mean\n",
        "\n",
        "        # Clipa os valores para garantir que estejam no intervalo [0, 1]\n",
        "        imagem_denormalizada = torch.clamp(imagem_denormalizada, 0, 1)\n",
        "\n",
        "        # Converte de tensor para PIL Image (espera CHW, Converte para HWC e depois para PIL)\n",
        "        imagem_transformada_pil = transforms.ToPILImage()(imagem_denormalizada)\n",
        "\n",
        "        print(\"\\nImagem Após Transformações de Validação:\")\n",
        "        display(imagem_transformada_pil)\n",
        "\n",
        "\n",
        "        # 4. Preparar o tensor para o modelo\n",
        "        imagem_tensor = imagem_tensor.unsqueeze(0)\n",
        "        imagem_tensor = imagem_tensor.to(device)\n",
        "\n",
        "        # 5. Colocar o modelo em modo de avaliação\n",
        "        model.eval()\n",
        "\n",
        "        # 6. Passar a imagem pelo modelo e 7. Interpretar a saída\n",
        "        with torch.no_grad(): # Desabilita o cálculo de gradientes para inferência (mais rápido e economiza memória)\n",
        "            output = model(imagem_tensor)\n",
        "\n",
        "            # A saída do modelo são logits.\n",
        "            # aplicamos Sigmoid para obter a probabilidade e depois arredondamos para a classe\n",
        "            probability = torch.sigmoid(output).item() # .item() pega o valor escalar do tensor\n",
        "\n",
        "            # Se a probabilidade for >= 0.5, prevemos 1 (Cachorro), caso contrário 0 (Gato)\n",
        "            predicted_class_index = round(probability)\n",
        "\n",
        "            # Mapear o índice de volta para o nome da classe\n",
        "            predicted_class_name = \"Dog\" if predicted_class_index == 1 else \"Cat\"\n",
        "\n",
        "        # Exibir o resultado\n",
        "        print(f\"\\nAnálise da imagem: {caminho_da_sua_foto}\")\n",
        "        print(f\"Probabilidade (Dog): {probability:.4f}\")\n",
        "        print(f\"Probabilidade (Cat): {1 - probability:.4f}\") # Probabilidade de Cat é 1 - Probabilidade de Dog\n",
        "        print(f\"O modelo prevê que a imagem é um: {predicted_class_name}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erro: Arquivo não encontrado em {caminho_da_sua_foto}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro ao processar a imagem: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ItN5O7daEX40",
        "HhoePKM-FBIG",
        "IkaE4d0Zh0pr",
        "0NjJPssgh1Zc",
        "Nz1To4crA3yc",
        "7H0nBi4qrK3b",
        "YfHjhqZprK3d",
        "bW7_pwcerK3f",
        "iwzMiXHprK3g"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}